# Image Classification Robustness Comparison

---

# Table of Contents

1. [Overview](#overview)
2. [Datasets & Models](#datasets--models)  
3. [Project Structure](#project-structure)  
4. [Setup & Usage](#setup--usage)  
5. [Training Configuration](#training-configuration)  
6. [Results & Evaluation](#results--evaluation)

---

# Overview

ë³¸ ì‹¤í—˜ì€ CNN ê¸°ë°˜ ëª¨ë¸ê³¼ Transformer ê¸°ë°˜ ëª¨ë¸ì´ ì´ë¯¸ì§€ ë¶„ë¥˜ ê³¼ì œì— ìˆì–´ robustness(ê°•ê±´ì„±)ì—ì„œ ì–´ë–¤ ì°¨ì´ë¥¼ ë³´ì´ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹¤í—˜ìœ¼ë¡œ, image classification pipeline êµ¬ì¶• ê³¼ì •ì˜ ê²½í—˜ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.

ìš°ì„ ì ìœ¼ë¡œ ì´ë¯¸ì§€ ì „ë°˜ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œ ìƒí™©ì—ì„œì˜ robustnessë¥¼ í‰ê°€í•œë‹¤. Transformer ê¸°ë°˜ ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬ CNN ê¸°ë°˜ ëª¨ë¸ë“¤ì€ ì˜ˆì¸¡ ì‹œ ì´ë¯¸ì§€ ì§ˆê°(texture)ì— ë§ì€ ì˜í–¥ì„ ë°›ëŠ”ë‹¤ëŠ” ì—°êµ¬ê°€ ìˆì–´ ì´ëŸ¬í•œ ë…¸ì´ì¦ˆì— ë¯¼ê°í•  ê²ƒì´ë¼ ì˜ˆìƒë˜ëŠ”ë°, ì‹¤í—˜ì„ í†µí•´ ì´ë¥¼ ê²€ì¦í•  ê²ƒì´ë‹¤. ë”ë¶ˆì–´ fine-tuningì„ í†µí•´ ê¸°ì¡´ ëª¨ë¸ë“¤ì´ ê°–ëŠ” ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆì„ì§€ í™•ì¸í•´ë³´ê³ ì í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆê°€ ë¼ì–´ ìˆëŠ” ê²½ìš° ì„±ëŠ¥ í•˜ë½ì´ ë°œìƒí•˜ëŠ” ì›ì¸ì„ ê·œëª…í•˜ê¸° ìœ„í•´ ë°°ê²½ ì˜ì¡´ë„ë¥¼ ì¸¡ì •í•˜ì—¬, ëª¨ë¸ì´ ê°ì²´ì˜ ì „ì²´ì ì¸ í˜•íƒœê°€ ì•„ë‹Œ ì£¼ë³€ ë§¥ë½ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ëŠ”ì§€ íŒŒì•…í•˜ê³ ì í•œë‹¤.

### Key Hypothesis
1. ì§ˆê°ë³´ë‹¤ ëª¨ì–‘ì— ì´ˆì ì„ ë§ì¶˜ ëª¨ë¸ì¸ Transformer ê¸°ë°˜ì˜ ëª¨ë¸ì´ CNN ê¸°ë°˜ì˜ ëª¨ë¸ë³´ë‹¤ robustnessê°€ ë†’ì„ ê²ƒì´ë©°, pretrainedë˜ì–´ ìˆì„ìˆ˜ë¡ ë” ê°•ê±´ì„±ì´ ë†’ì„ ê²ƒì´ë‹¤.
2. Fine-tuningì„ ì§„í–‰í•  ì‹œ ëª¨ë¸ ì„±ëŠ¥ì´ ì „ë°˜ì ìœ¼ë¡œ í–¥ìƒë  ê²ƒì´ë‹¤. ê¸°ì¡´ì— ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë˜ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒ ì •ë„ê°€ ê°€ì¥ í¬ê²Œ ë‚˜íƒ€ë‚  ê²ƒì´ë‹¤.
3. ë¬¼ì²´ê°€ ì•„ë‹Œ ë°°ê²½ì— ëŒ€í•œ ì˜ì¡´ë„ëŠ” CNN ê¸°ë°˜ì˜ ëª¨ë¸ì´ Transformer ê¸°ë°˜ì˜ ëª¨ë¸ë³´ë‹¤ ë†’ì„ ê²ƒì´ë‹¤. ì¦‰, CNN ê¸°ë°˜ ëª¨ë¸ì˜ ë°°ê²½ ì˜ì¡´ë„ê°€ ë” ë†’ì„ ê²ƒì´ë‹¤.

---

# Datasets & Models

Datasets:

- **CIFAR-10**: ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©í•  ê¸°ë³¸ ë°ì´í„°ì…‹
- **CIFAR-10-C**: CIFAR ë°ì´í„°ì…‹ì— ì´ 19ê°€ì§€ì˜ corruptionì´ ì ìš©ëœ ë°ì´í„°ì…‹

> brightness, contrast, defocus blur, elastic, fog, frost, frosted glass blur, gaussian blur, gaussian noise, impluse noise, jpeg compression, motion blur, pixelate, saturate, shot_noise, snow, spatter, speckle noise, zoom blur

- **ImageNet-9**: ë°°ê²½ ì˜ì¡´ì„±ê³¼ ê°•ê±´ì„± íŒŒì•…ì„ ìœ„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ, ImageNetì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ 9ê°œ ìƒìœ„ í´ë˜ìŠ¤ë§Œ ë½‘ê³  ë¬¼ì²´ê°€ ì•„ë‹Œ ë°°ê²½ì„ ë°”ê¾¼ ë°ì´í„°ì…‹

> `original`(Training ìš©), `mixed_rand`(ë°°ê²½ì„ ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ ë°°ê²½ìœ¼ë¡œ ëœë¤í•˜ê²Œ ë°”ê¾¼ ê²ƒ, Test ìš©), `only_fg`(ë°°ê²½ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ ì§€ìš°ê³  ë¬¼ì²´ë§Œ ë‚¨ê¸´ ê²ƒ, ë³´ì¡°)


Models: 
#### CNN-based Models:
- ResNet50 w/o pre-trained weights
- ResNet50 w/ pre-trained on ImageNet 1k

#### Transformer-based Models:
- ViT-S/16 w/o pre-trained weights
- ViT-S/16 w/ pre-trained on ImageNet 1k

---

# Project Structure

```
CV
â”œâ”€â”€ README.md
â”œâ”€â”€ configs
â”‚   â”œâ”€â”€ defaults.yaml
â”‚   â””â”€â”€ models
â”‚       â”œâ”€â”€ resnet50.yaml
â”‚       â”œâ”€â”€ resnet50_pretrained.yaml
â”‚       â”œâ”€â”€ resnet50_pretrained_in9.yaml
â”‚       â”œâ”€â”€ vit_small.yaml
â”‚       â”œâ”€â”€ vit_small_pretrained.yaml
â”‚       â””â”€â”€ vit_small_pretrained_in9.yaml
â”œâ”€â”€ data/                         
â”‚   â”œâ”€â”€ cifar10/
â”‚   â”œâ”€â”€ CIFAR-10-C/
â”‚   â””â”€â”€ ImageNet9/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ factory.py 
â”œâ”€â”€ eval_background_robustness.py
â”œâ”€â”€ eval_robustness.py
â”œâ”€â”€ finetune.py
â”œâ”€â”€ run_background_all.sh
â”œâ”€â”€ run_finetune_all.sh
â”œâ”€â”€ train.py
â””â”€â”€ utils
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ data_loader.py
    â”œâ”€â”€ logger.py
    â””â”€â”€ metrics.py
```

# Setup & Usage


### Quick Start

**Step 1. Install dependencies**
```bash
pip install -r requirements.txt
```

**Step 2. Prepare datasets**

Place your dataset files in the following path:
```bash
/CV/data
```

**Step 3. Set WandB API key**
```bash
wandb login [API_KEY]
```


# Training Configuration

ê° ëª¨ë¸ì— ì‚¬ìš©ëœ í•™ìŠµ ì„¤ì •ì€ ê´€ë ¨ ëª¨ë¸ë³„ YAML íŒŒì¼ì—ì„œ í™•ì¸ ê°€ëŠ¥

#### Data
- Datasets: CIFAR-10, CIFAR-10-C, ImageNet-9
- Input Size: 32x32 for CNN-based models, 224Ã—224 for Transformer based models
- Epochs: 50 for models w/o pretrained weights, 20 for models w/ pretrained weights
- Batch Size: 64  
- Loss Function: Cross Entropy Loss  

#### Models

- CNN-based: `resnet50`  
    - w/o pretrained weights
        - learning_rate: 0.1
        - weight_decay: 5e-4
        - optimizer: "sgd"
        - momentum: 0.9
    - w/ pretrained weights
        - learning_rate: 0.01
        - weight_decay: 5e-4
        - optimizer: "sgd"
        - momentum: 0.9
- ViT-based: `vit_small_patch16_224`
    - w/o pretrained weights
        - learning_rate: 1e-4
        - weight_decay: 0.05
        - optimizer: "adamw"
    - w/ pretrained weights
        - learning_rate: 1e-5
        - weight_decay: 1e-4
        - optimizer: "adamw"

> `pretrained` value: `True` or `False`

ì„ íƒëœ ìµœì í™” íŒŒë¼ë¯¸í„°ë“¤ì€ í•´ë‹¹ ëª¨ë¸ì´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ë¡œ ìµœëŒ€í•œ ì„¤ì •í•¨

#### Evaluation Metrics

- Accuracy (Top-1)  
- Accuracy (Top-5)

# Results & Evaluation

### Experiment 1. Comparison of Overfitting Patterns

- Dataset: CIFAR-10
    - Train set: train(90%)/validation(10%) setìœ¼ë¡œ ë¶„ë¦¬
    - Test
- Models: ResNet50 w/o pre-trained weights(`resnet50`), ResNet50 w/ pre-trained on ImageNet 1k(`resnet50_pretrained`), ViT-S/16 w/o pre-trained weights(`vit_small`), ViT-S/16 w/ pre-trained on ImageNet 1k(`vit_small_pretrained`)


#### Run all training jobs
```bash
bash run_all.sh
```
Experiment 2ëŠ” Experiment 1ì—ì„œ í›ˆë ¨ëœ ìµœì  ëª¨ë¸ì— ê¸°ë°˜í•´ robustnessë¥¼ í‰ê°€í•˜ê¸° ë•Œë¬¸ì— ì´ ì‘ì—…ì€ Experiment 1 & 2ë¥¼ ë™ì‹œì— ì‹¤í–‰

#### Output

The results will be saved to:

```
/CV/checkpoints/{model_name}_{data_name}
```

Saved files include:

| File                        | Description                         |
|-----------------------------|-------------------------------------|
| `best_model.pth`            | Best model (based on validation)   |
| `data_config.yaml`             | Snapshot of data configs used            |
| `model_config.yaml`             | Snapshot of model configs        |


### Result

| model | val_acc(top1)* | val_acc(top5)* | val_acc*@epoch | test_loss | test_acc(top1) | test_acc(top5) |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| resnet50 | 70.46% | 96.98% | 38 | 0.9025 | 69.98% | 96.94% |
| resnet50_pretrained | 83.38% | 99.2% | 20 | 0.6291 | 82.34% | 98.99% |
| vit_small | 71.72% | 97.1% | 50 | 1.4487 | 71.39% | 97.17% |
| **vit_small_pretrained** | 98.24% | 99.96% | 6 | 0.0914 | **98.01%** | **99.97%** |

<div align="center">
  <table width="100%">
    <tr>
      <td width="50%" align="center">
        <img src="https://github.com/user-attachments/assets/57a768e5-a5cf-4b53-9d36-8e6f726dee11" width="100%" />
        <br>
        Fig 1. resnet50
      </td>
      <td width="50%" align="center">
        <img src="https://github.com/user-attachments/assets/f4b7e6e6-e529-4c55-91bc-690a3b5779bf" width="100%" />
        <br>
        Fig 2. resnet50_pretrained
      </td>
    </tr>
  </table>
</div>


<div align="center">
  <table width="100%">
    <tr>
      <td width="50%" align="center">
        <img src="https://github.com/user-attachments/assets/e05cc4e9-4ace-40b4-8ec8-837322ba2f14" width="100%" />
        <br>
        Fig 3. vit_small
      </td>
      <td width="50%" align="center">
        <img src="https://github.com/user-attachments/assets/f4b7e6e6-e529-4c55-91bc-690a3b5779bf" width="100%" />
        <br>
        Fig 4. vit_small_pretrained
      </td>
    </tr>
  </table>
</div>

> Best epochì€ top-1 validation accuracyë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„ ì •ë˜ë©°, í•´ë‹¹ ì‹œì ì˜ ëª¨ë¸ì´ best modelë¡œ ì €ì¥ë¨

---

### Experiment 2. Comparison of Robustness

- Dataset: CIFAR-10-C
    - 19ê°œì˜ corruption ì¢…ë¥˜, 5ê°œì˜ severity ì •ë„ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ ì´ 90 ì¢…ë¥˜ì˜ ì¡°í•©ì— ëŒ€í•´ ê°ê° í‰ê°€ë¥¼ ìˆ˜í–‰í•œë‹¤. ë³¸ ë¬¸ì„œì—ì„œëŠ” 90 ì¢…ë¥˜ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼ì˜ í‰ê·  ì •í™•ë„ë¥¼ ì²¨ë¶€í•œë‹¤.
- Models: ResNet50 w/o pre-trained weights(`resnet50`), ResNet50 w/ pre-trained on ImageNet 1k(`resnet50_pretrained`), ViT-S/16 w/o pre-trained weights(`vit_small`), ViT-S/16 w/ pre-trained on ImageNet 1k(`vit_small_pretrained`)


#### Output

The results will be saved to:

```
/CV/checkpoints/{model_name}_{data_name}
```

| File                        | Description                         |
|-----------------------------|-------------------------------------|
| `robustness_results.csv`            | top1_acc, top5_acc for all corruption, severity pairs   |


### Result 

| model | mean_acc(top1) | mean_acc(top5) |
| :--- | :---: | :---: |
| resnet50 | 58.47% | 92.50% |
| resnet50_pretrained | 70.52% | 96.69% |
| vit_small | 58.80% | 93.29% |
| **vit_small_pretrained** | **89.81%** | **98.69%** |

*Hypothesis 1*: ì§ˆê°ë³´ë‹¤ ëª¨ì–‘ì— ì´ˆì ì„ ë§ì¶˜ ëª¨ë¸ì¸ Transformer ê¸°ë°˜ì˜ ëª¨ë¸ì´ CNN ê¸°ë°˜ì˜ ëª¨ë¸ë³´ë‹¤ robustnessê°€ ë†’ì„ ê²ƒì´ë©°, pretrainedë˜ì–´ ìˆì„ìˆ˜ë¡ ë” ê°•ê±´ì„±ì´ ë†’ì„ ê²ƒì´ë‹¤. â˜‘ï¸


---

### Experiment 3. Comparison of Improvement due to Fine-tuning

- Dataset: CIFAR-10-C
    - ëª¨ë“  Corruptionì˜ ì•ìª½ 90% ë°ì´í„°ë¥¼ train ë° validation set, ë’¤ìª½ 10% ë°ì´í„°ë¥¼ test setìœ¼ë¡œ ì‚¬ìš©í•˜ë©°, ì•ìª½ 90% ë°ì´í„° ì•ˆì—ì„œ train(90%)/validation(10%) setìœ¼ë¡œ ë¶„ë¦¬í•œë‹¤.
- Models: ResNet50 w/o pre-trained weights(`resnet50`), ResNet50 w/ pre-trained on ImageNet 1k(`resnet50_pretrained`), ViT-S/16 w/o pre-trained weights(`vit_small`), ViT-S/16 w/ pre-trained on ImageNet 1k(`vit_small_pretrained`)
- Epoch: 5

#### Run all training jobs
```bash
bash run_finetune_all.sh
```

#### Output

The results will be saved to:

```
/CV/checkpoints/{model_name}_{data_name}
```

| File                        | Description                         |
|-----------------------------|-------------------------------------|
| `finetune_results.csv`            | test top1_acc, top5_acc for all corruption, severity pairs calculated w/ the fine-tuned model   |


### Result 

| model | mean_acc(top1) | mean_acc(top5) | improvement(top1) compared to Exp.2 |
| :--- | :---: | :---: | :--: |
| resnet50 | 98.97% | 99.90% | +40.5%p
| resnet50_pretrained | 99.62% | 99.99% | +29.1%p
| vit_small | 99.99% | 99.99% | **+41.19%p**
| vit_small_pretrained | 99.99% | 99.99% | +10.18%p

*Hypothesis 2*: Fine-tuningì„ ì§„í–‰í•  ì‹œ ëª¨ë¸ ì„±ëŠ¥ì´ ì „ë°˜ì ìœ¼ë¡œ í–¥ìƒë  ê²ƒì´ë‹¤. ê¸°ì¡´ì— ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë˜ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒ ì •ë„ê°€ ê°€ì¥ í¬ê²Œ ë‚˜íƒ€ë‚  ê²ƒì´ë‹¤. ğŸ”º

ì‹¤ì œë¡œ 4ê°€ì§€ ëª¨ë¸ì´ ì „ë¶€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í•  ìˆ˜ ìˆì–´, fine-tuning ì‹œ CNN ê¸°ë°˜, Transformer ê¸°ë°˜ ëª¨ë¸ì˜ robustness ì°¨ì´ê°€ ê±°ì˜ ì‚¬ë¼ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤. 


---

### Experiment 4. Background Robustness

- Dataset: ImageNet-9
    - original: ì›ë³¸ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ (Training ìš©)
    - mixed_rand: ë°°ê²½ì„ ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ ë°°ê²½ìœ¼ë¡œ ëœë¤í•˜ê²Œ ë°”ê¾¼ ë°ì´í„°ì…‹ (Test ìš©)
    - only_fg: ë°°ê²½ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ ì§€ìš°ê³  ë¬¼ì²´ë§Œ ë‚¨ê¸´ ë°ì´í„°ì…‹ (ë³´ì¡°)
    - `background_gap` = `original(test)_acc` - `mixed_rand_acc` ë¥¼ í†µí•´ ë°°ê²½ ì˜ì¡´ë„ ì¸¡ì •
- Models: ResNet50 w/ pre-trained on ImageNet 1k(`resnet50_pretrained`), ViT-S/16 w/ pre-trained on ImageNet 1k(`vit_small_pretrained`)
    - ImageNet ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸°ì— ì í•©í•œ pretrained ëª¨ë¸ë§Œ ì‚¬ìš©
- Epoch: 5


#### Run all training jobs
```bash
bash run_background_all.sh
```

### Result 

| model | original_acc(test) | mixed_rand_acc | only_fg_acc | background gap
| :--- | :---: | :---: | :--: | :--: |
| resnet50_pretrained | Top-1:Â 97.28%Â / Top-5:Â 100.00% | Top-1:Â 80.40%Â /Â Top-5:Â 98.22% | Top-1:Â 91.04%Â /Â Top-5:Â 99.21% | 16.89%p |
| **vit_small_pretrained** | Top-1:Â 98.52%Â /Â Top-5:Â 100.00% | Top-1:Â 87.41%Â /Â Top-5:Â 98.72% | Top-1:Â 93.85%Â /Â Top-5:Â 99.36% | **11.11%p**

*Hypothesis 3*: ë¬¼ì²´ê°€ ì•„ë‹Œ ë°°ê²½ì— ëŒ€í•œ ì˜ì¡´ë„ëŠ” CNN ê¸°ë°˜ì˜ ëª¨ë¸ì´ Transformer ê¸°ë°˜ì˜ ëª¨ë¸ë³´ë‹¤ ë†’ì„ ê²ƒì´ë‹¤. ì¦‰, CNN ê¸°ë°˜ ëª¨ë¸ì˜ ë°°ê²½ ì˜ì¡´ë„ê°€ ë” ë†’ì„ ê²ƒì´ë‹¤. â˜‘ï¸

---

> ### ê°•ê±´ì„±ì€ Transformer ê¸°ë°˜ ëª¨ë¸ì´ CNN ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ë” ì¢‹ìœ¼ë©°, Transformer ê¸°ë°˜ ëª¨ë¸ì€ ë°°ê²½ì´ ì•„ë‹Œ ì‚¬ë¬¼ ìì²´ì— ë” ì§‘ì¤‘í•˜ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚œë‹¤.

